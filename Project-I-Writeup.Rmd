---
title: "Part I Write-up"
team: "Team-10"
author: "Jingyi Zhang, Jonathan Klus, Bin Han"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r load_packages, message=FALSE, warning=FALSE, echo =FALSE}
suppressWarnings(library(knitr)) 
suppressWarnings(library(dplyr))
suppressWarnings(library(ggplot2))
suppressWarnings(library(GGally))
suppressWarnings(library(mice))
suppressWarnings(library(purrr))
suppressWarnings(library(glmnet))
suppressWarnings(library(MASS))
suppressWarnings(library(BAS))
```


```{r read_data, echo=FALSE}
set.seed(10)
load("paintings_train.Rdata")
load("paintings_test.Rdata")
```

## 1. Introduction:

In this study, the auction prices of paintings in 18th century Paris were examined. Specifically, we wish to understand the variables which affect the prices of the paintings, and then be able to predict auction prices based on characteristics of a certain painting. By fitting an appropriate model, we will also be creating a tool to help decide whether specific paintings that are either underpriced or overpriced given their realization of the covariates that were included in the model.

One of the main challenges in building this model is to narrow down the number of covariates from the 59 canadidates in the original data set to less than 20 in the final model. This must be done in such a way that an undue amount of bias is not introduced, and overfitting is avoided. The ability to explain the results and provide some recommendations to indivisuals without statistical background is equally important and challenging, since the primary audience for this analysis is intended to be art historians. The goal was therefore to balance predictive performance, model simplicity, and interprebility in order to create a pricing model for artwork in 18th century France.


## 2. Exploratory data analysis:

## A) Data summary & cleaning

The training set consists of a few numeric variables and many categorical variables. Some variables, such as `Interm`, `Surface`, `Height_in` etc. have mising values, which need to be imputed before any analysis can occur. Data cleaning proceeded using the following steps:

```{r useless, echo = FALSE}
useless_var = paintings_train %>% 
  dplyr::select(lot,
         sale,
         price,
         count,
         subject,
         authorstandard,
         author,
         winningbidder,
         other)
```



```{r useless_sum}
t(summary(useless_var))

m = data.frame(
  variables = colnames(useless_var),
  unique_values = unlist(map(1:ncol(useless_var), function(i) length(unique(useless_var[,i]))))
)
kable(m, align = "c")
```


a. In order to reduce the dimensionality of the problem, variables that were deemed intuitively not useful or colinear with other covariates were removed. This included: `lot`, `sale`, `price`, `count`, `subject`, `authorstandard`, `author`, `winningbidder`, and `other`. From the and structure and summary table, the `count` variable has all 1's; the `other` variable does not convey useful information; the other variables, such as `names` and `subjects`, are not useful in their present form for predicting the response variable. While its possible that some variables like `subjects` could be recoded using some underlying characteristic (and given some art expertise), we do not attempt to do this here.

b. It was determined that `Surface` and `Surface_Rnd`, `Surface_Rect` are similar, based on the value of `Height_in`, `Width_in`, and `Diam_in`. We decided to use `Surface` in our initial model as it contained all information in the latter two surface area variables. A similar approach was used for variables `material`, `mat`, and `materialCat`. The latter one recodes the previous one, with fewer levels for simplicity (39 levels in the former versus just 5 in the latter). Therefore, we used `materialCat`. We applied the same strategy to keep `landsALL` and remove other `lands` indicator variables which contained little information and would therefore have been difficult to estimate an accurate coefficient. 

c. This data contained a great deal of structurally missing values (i.e. missingness resulting from how the researchers coded the data, rather than truly unavailable or omitted information). For those variables that have multiple levels, to be consistent with how the data was originally coded, we recoded the missing levels as "X", which stands for either "other" or "no information" in the code book, depending upon the variable in question. For `materialCat` and `Shape`, since there are so many levels, we grouped some levels with few (<10) observations together, coded as the "other" group. The remaining binary vairables were converted into factors.

d. The remaining data issue was how to deal with missing values in the numeric continuous variable `Surface` and the binary variable `Interm`. The `mice` package (Multivariate Imputation by Chained Equations) was used to address this problem. It uses the observed values of other covariates in the dataset to create a model to impute the missing values. This method is superior to complete case analysis, which would result in losing an unacceptably large amount of data, as well as simpler imputation methods (i.e. imputing the mean of a given covariate to replace missing values).

```{r data_clean, echo=FALSE}
## Remove not useful & colinear variables
paintings_train_1 = paintings_train %>% 
  dplyr::select(-sale,
                -price,
                -count,
                -subject,
                -authorstandard,
                -author,
                -winningbidder,
                -other,
                -Height_in,
                -Width_in,
                -Surface_Rect,
                -Diam_in,
                -Surface_Rnd,
                -material,
                -mat,
                -lands_sc,
                -lands_elem,
                -lands_figs,
                -lands_ment,
                -lot,
                -type_intermed) %>% 
  mutate(
    dealer = as.factor(dealer),
    origin_author = as.factor(origin_author),
    origin_cat = as.factor(origin_cat),
    school_pntg = as.factor(school_pntg),
    authorstyle = ifelse(authorstyle %in% c("n/a", ""), 0, 1) %>% as.factor(),
    winningbiddertype = ifelse(winningbiddertype %in% c("n/a", ""), "X", winningbiddertype) %>% as.factor(),
    endbuyer = ifelse(endbuyer %in% c("n/a", ""), "X", endbuyer) %>% as.factor(),
    materialCat = ifelse(materialCat %in% c("n/a", ""), "other", materialCat) %>% as.factor(),
    Shape = ifelse(Shape %in% c("round", "roude"), "round",
                   ifelse(Shape %in% c("oval", "ovale"), "oval",
                          ifelse(Shape == "squ_rect", "squ_rect", "other"))) %>% as.factor(),
    artistliving = as.factor(artistliving),
    diff_origin = as.factor(diff_origin),
    engraved = as.factor(engraved),
    original = as.factor(original),
    prevcoll = as.factor(prevcoll),
    othartist = as.factor(othartist),
    paired = as.factor(paired),
    figures = as.factor(figures),
    lrgfont = as.factor(lrgfont),
    relig = as.factor(relig),
    landsALL = as.factor(landsALL),
    arch = as.factor(arch),
    mytho = as.factor(mytho),
    peasant = as.factor(peasant),
    othgenre = as.factor(othgenre),
    singlefig = as.factor(singlefig),
    portrait = as.factor(portrait),
    still_life = as.factor(still_life),
    discauth = as.factor(discauth),
    history = as.factor(history),
    allegory = as.factor(allegory),
    pastorale = as.factor(pastorale),
    finished = as.factor(finished)
  ) %>%  
  .[,c(8, 1:7, 9:38)]

```

```{r imputation, message=FALSE, warning=FALSE, echo = FALSE}
micetest = mice::mice(paintings_train_1, printFlag = FALSE)
paintings_train_2 = mice::complete(micetest) %>% 
  mutate(Interm = as.factor(Interm))

```


## B). Plots
Following the data cleaning and dimension reduction described above, the potential covariates were plotted against the response variable, `logprice`. Scatter plots were used for numeric variables, and allow us to roughly determine which variables may be useful in the initial model. For categorical variables, we use boxplots to check if the range of `logprice` is observably different for each level of the variable. If so, it may be a good predictor. For numeric variables, we want to check if there is a clear linear relationship between the variable and `logprice`.  
 
```{r parse_df, message=FALSE, warning=FALSE, echo=FALSE}
graph_numeric = paintings_train_2 %>% 
  dplyr::select(position,
         year,
         Surface,
         nfigures)

graph_categorical = paintings_train_2 %>% 
  dplyr::select(-position,
         -year,
         -Surface,
         -nfigures,
         -logprice)
```

```{r num_eda}
## numeric
par(mfrow = c(2, 2))
for (i in 1:ncol(graph_numeric)){
  plot(y = paintings_train_2$logprice, 
       x = graph_numeric[,i],
       ylab = "logprice",
       xlab = names(graph_numeric)[i])
}

```

For numeric variables, we note that `Surface` and `nfigures` appear to have a weak but positive relationship with `logprice`. Since there are several extremely large values in `position` (potential outliers), it is difficult to know if there is a truly useful relationship here between the majority of points and `logprice`. But we will keep it in the initial model for now.

```{r cat_eda}
## categorical
par(mfrow = c(2,3))
for (i in 1:12){
  boxplot(paintings_train_2$logprice ~ graph_categorical[,i],
          ylab = "logprice",
          xlab = names(graph_categorical)[i])
}

```

Since there are 33 categorical variables, we don't show the boxplots for all of them. But applied the same method to check all the categorical variables. The following variables show some differences in `logprice` at different levels (not considering the magnitude of the difference at this time): `dealer`, `origin_author`, `origin_cat`, `school_pntg`, `diff_origin`, `authorstyle`, `endbuyer`, `Interm`, `Shape`, `materialCat`, `engraved`, `prevcoll`, `figures`, `finished`, `Irgfont`, `othgenre`, `discauth`, and `still_life`.

If we were to choose 10 best variables for prediction at this point, we would consider the magnitude of differences and the strength of relationships. The 10 best variables based upon the above EDA are: `Surface`, `dealer`, `school_pntg`, `diff_origin`, `authorstyle`, `endbuyer`, `Interm`, `prevcoll`, `engraved`, `Irgfont`. They exhibit the strongest relationship with the response variable, `logprice`.

## 3. Development and assessment of an initial model

```{r bma_init, echo = FALSE}
## JZS prior
bma1 = bas.lm(logprice~ ., 
             data=paintings_train_2, 
             method="MCMC", 
             prior = "JZS",
             modelprior=uniform(),
             n.models = 15000, MCMC.iterations=100000, 
             thin = 10, initprobs="marg-eplogp",
             force.heredity=FALSE)
#plot(bma1, which=4)
BPM1 = predict(bma1, estimator = "BPM")

## g-prior
bma2 = bas.lm(logprice~ ., 
             data=paintings_train_2, 
             method="MCMC", 
             prior = "g-prior",
             modelprior=uniform(),
             n.models = 15000, MCMC.iterations=100000, 
             thin = 10, initprobs="marg-eplogp",
             force.heredity=FALSE)

#plot(bma2, which=4)
BPM2 = predict(bma2, estimator = "BPM")
```

## Initial Model

### JZS prior
```{r bma_1}
variable.names(BPM1)
```

### g-prior
```{r bma_2}
variable.names(BPM2)
```

The EDA process gives us an initial idea of which variables to drop, and which variables might be important to explaining the variation in logprice. But before we built the initial model, we applied BMA ( Bayesian Model Averaging), to systematically choose which base variables have the highest posterior probabilities of being included in the initial model. We experimented with two priors, "JZS" and "g-prior", which gave us two sets of variables listed above. Then we picked up the common ones from the Best Predictive Model (BPM).

An OLS regression model was then fit using the chosen features and all their possible interactions. From the summary table, the $R^{2} = 0.5828$, was okay, with approximately 58% of the variation in `logprice` explained by the model. But this model suffered from several potential issues, not least among which was that many coefficients could not be estimated (they returned NAs). Despite attempts during the first part of EDA at dimension reduction and removing colinear variables, it appears that multicolinearity is still very clearly an issue, as our design matrix is not full rank. Therefore, we need to again reduce the dimensionality of the model through further variable selection.

```{r init_ols, warning=FALSE, message=FALSE}
ols = lm(logprice ~ dealer + school_pntg + diff_origin + artistliving + endbuyer + authorstyle + 
                       Interm + Shape + Surface + engraved + prevcoll + paired + 
                       finished + lrgfont + portrait + discauth + still_life, 
         data = paintings_train_2)
summary(ols)
```


## Model Selection

After completing the initial exploratory data analysis, methods including Stepwise Selection using both AIC and BIC penalties were used in order to assess more systematically which covariates and interactions were most important for predicting the `logprice` of paintings. While the number of relevant covariates was initially thinned by examining the data and determining which variables were best suited for modeling (e.g. via dimension reduction, elimination or recoding of categorical variables with too many levels or too few observations for a given level to be useful in estimating a coefficient), there still remained a large number of covariates from which to choose. The goal in using the above described methodology was to demonstrate among several methods, both frequentist and Bayesian, which covariates were routinely deemed to be the most important for modeling logprice. 

The variable selection methods described above remain computationally intensive, particularly given the number of variables and potential two-way interactions that must be considered. In order to begin the analysis, The two-way interactions were considered using stepwise selection (AIC & BIC). The goal of this penalized selection process was to avoid overfitting by reducing the number of covariates included in the final model and to deliver a model that was both interpretable and performed well in prediction. The results of the two methods were then compared, and interactions that were not intuitive were filtered out (e.g.  $artistling * endbuyer$).

Ultimately, the following variables were selected using the above methods and were fit using OLS regression. The resulting $Adj-R^{2}$ was 0.6079. All the included covariates had estimable coefficients (i.e. there were no NAs, as the resulting design matrix was full rank). \par


```{r final_ols, message=FALSE, warning=FALSE}
ols.2 = lm(logprice ~ Shape + school_pntg + dealer*Interm + dealer*Surface + dealer*paired + dealer*finished + diff_origin*Surface + diff_origin*portrait + artistliving*endbuyer + Interm*Surface + Interm*lrgfont + Surface*lrgfont + Surface*still_life + Surface*discauth + prevcoll*finished + paired*lrgfont + paired*discauth + diff_origin*authorstyle + diff_origin*still_life + finished*discauth + lrgfont*discauth + artistliving*finished + Interm*portrait + dealer*artistliving + authorstyle*prevcoll, data = paintings_train_2)

summary(ols.2)
```


## Residuals & Diagnostics Analysis
```{r message=FALSE, warning=FALSE}
par(mfrow = c(2,2))
plot(ols.2)
```

After fitting the model, we created the four model diagnostic plots. The overall appearances of all four plots appear acceptable, with no obvious outliers or highly influential points shown. The model also does not violate the normality assumption for residuals. The constant variance of residuals assumption appears to be satisfied, and there is no fanning or other obvious pattern in this plot. While there are a two points that are identified as outliers, they were not found to be influential.

## Variables
```{r}
table_of_coef = cbind(coef(ols.2), confint(ols.2))
colnames(table_of_coef) = c("Coefficient", "2.5%", "97.5%")
kable(table_of_coef, digits = 3, align = "c")
```

In the linear model we selected, we included `Shape`, `school_pntg`, `dealer`, `Interm`, `Surface`, `paired`, `finished`, `discauth`, `diff_origin`, `portrait`, `artistliving`, `endbuyer`, `authorstyle`, `lrgfont`, `still_life`, and `prevcoll` as our base predictors. Interactions selected by the model selection process and, for the sake of interpretation, those that are reasonable and interpretable are kept in the model as well. Since the response variable was orginally log-transformed, the model is interpreted in terms of exponentiated values below.

## 4. Summary and Conclusions
```{r, echo=FALSE}
new = data.frame(
  Shape = "other",
  school_pntg = "A",
  dealer = "J",
  Interm = "0",
  Surface = median(paintings_train_2$Surface),
  paired = "0",
  finished = "0",
  discauth = "0",
  diff_origin = "0",
  portrait = "0",
  artistliving = "0",
  endbuyer = "B",
  authorstyle = "0",
  lrgfont = "0",
  still_life = "0",
  prevcoll = "0"
)
```


a. The median price predicted is `exp(4.401232) = 81.55128` livres. The 95% confidence interval is (6.248, 1064.357) livres. The prediction interval is (2.532, 2626.714) livres.
```{r}
kable(exp(predict(ols.2, newdata = new, interval = "confidence")), 
      digits = 3, 
      align = "c",
      caption = "95% Confidence Interval")
      
kable(exp(predict(ols.2, newdata = new, interval = "prediction")), 
      digits = 3, 
      align = "c",
      caption = "95% Prediction Interval")

```


## Interpretation
From the final model, the following variables are statistically significant: `dealer`, `Interm`, `Surface`, `finished`, `discauth`, `diff_origin`, `portrait`, `endbuyer (E,U, X)`, `authorstyle`, `lrgfont`, and `prevcoll`. Some of the interactions are statistically important, such as: `dealer*Interm`, `dealer*paried`, `Interm*lrgfont`, `diff_origin*portrait` etc. The most important covariates and interactions are interpreted as follows: 

* dealer: the type of dealer that the auction went through significantly affects the price of the painting. For example, compared with dealer J, the average price from dealer L is `244.5% higher`. (Same interpretation for dealer P and R, with different coefficients)

* Interm: when there is an intermediary involved in the transaction, on average, the selling price is `99% lower` than when there is no intermediary involved. 

* Surface: for every one square inch increase in the painting surface, the selling price is, on average expected to increase `.045%`.

* finished: if the painting is noted for being highly finished, the selling price on average is `130.3% higher` than when the painting is not noted for being highly finished.

* portrait: if the painting is described as a portrait, the selling price on average is `101.4% lower` times lower than when the painting is not described as a portrait. 

* endbuyer: the type of endbuyer will significantly affect the level of price. For instance, compared with the endbuyer type B (buyer), the average selling price is `-108.1% lower` when the endbuyer is type E (expert). 

* prevcoll: when the previous owner is mentioned, the average selling price is `151.0% higher` than when the previous owner is not mentioned.

* lrgfont: when the dealer devotes an additional paragraph, the average selling price is `178.0% higher` than when there is no additional paragraph.

* authorstyle: when the author's name is introduced, the average selling price is expected to be `200.4% lower` than when the author's name is not introduced.

* dealer&Interm interaction: when an intermediary is present, which the price of the auctioned paintings differs significantly among different dealers. For instance, if the dealer is R and an intermediary is used, the average selling price is `225.0% higher` than when the dealer is J with an intermediary.

* finished*prevcoll: given that the painting is noted for being highly finished, when the previous owner is mentioned, the average price is expected to be `110.4% lower` than when the previous owner is not mentioned.


## Recommendations
In order to understand the auction prices of 18th century paintings and predict prices of paintings with certain features, we recommend historians focusing on the characteristics mentioned above associated with the painitings (just to mention some, not a complete list). For example, in order to find out highly priced pieces, they might want to look for transactions that involved dealer R, with an intermediary involved; they might want to look for dealer L with the painting sold as a pair with another one; they should look for larger, finished paintings; they should focus on paintings whose authors' names are mentioned during the auction. These features were among those that conferred the greatest percent increase in price over the base case in the model presented above.


## Limitations
As mentioned in the data cleaning process, some of the variables have so many levels that fitting (and interpreting) such a model would be cumbersome. Many levels of the categorical variables included in this data set have few observations that are not sufficient for estimating coefficients. Therefore, we grouped some of the levels of variables, and grouped some variables via dimension reduction. This reduces the granularity of the analysis presented above, and risks introducing some bias, as the true model would probably be more granular and include the actual realization of each categorical predictor, not just an artbitrary grouping.

As our goal is to find a balance between the prediction accuracy and interpretability, our model may not predict the logprice response variable as accurately as other more advanced methods (which sacrifice interpretability in exchange for improved predictions). In the next phase of this project, we will attempt to fit other models which do just this, then compare the results to our initial work using an OLS regression model. 

