---
title: "Final Data Analysis Project"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "7 December 2018"
---

```{r setup, include=FALSE, echo =FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

### Read in Training Data
```{r message=FALSE, warning=FALSE, echo =FALSE}
suppressWarnings(library(knitr)) 
suppressWarnings(library(dplyr))
suppressWarnings(library(ggplot2))
suppressWarnings(library(GGally))
suppressWarnings(library(mice))
suppressWarnings(library(purrr))
suppressWarnings(library(glmnet))
suppressWarnings(library(MASS))
suppressWarnings(library(BAS))
```


```{r read-data, echo=FALSE}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
```


## Part I: Simple Model 

### EDA
## Data Summary
```{r, echo=FALSE}
t(summary(paintings_train))
str(paintings_train)

```

```{r, echo = FALSE}
useless_var = paintings_train %>% 
  dplyr::select(lot,
         sale,
         price,
         count,
         subject,
         authorstandard,
         author,
         winningbidder,
         other)
```

```{r}
t(summary(useless_var))

m = data.frame(
  variables = colnames(useless_var),
  unique_values = unlist(map(1:ncol(useless_var), function(i) length(unique(useless_var[,i]))))
)
kable(m, align = "c")
```

## Manually Data Cleaning
```{r, echo=FALSE}
## Remove Intuitively Useless Variables
paintings_train_1 = paintings_train %>% 
  dplyr::select(-sale,
                -price,
                -count,
                -subject,
                -authorstandard,
                -author,
                -winningbidder,
                -other,
                -Height_in,
                -Width_in,
                -Surface_Rect,
                -Diam_in,
                -Surface_Rnd,
                -material,
                -mat,
                -lands_sc,
                -lands_elem,
                -lands_figs,
                -lands_ment,
                -lot,
                -type_intermed) %>% 
  mutate(
    dealer = as.factor(dealer),
    origin_author = as.factor(origin_author),
    origin_cat = as.factor(origin_cat),
    school_pntg = as.factor(school_pntg),
    authorstyle = ifelse(authorstyle %in% c("n/a", ""), 0, 1) %>% as.factor(),
    winningbiddertype = ifelse(winningbiddertype %in% c("n/a", ""), "X", winningbiddertype) %>% as.factor(),
    endbuyer = ifelse(endbuyer %in% c("n/a", ""), "X", endbuyer) %>% as.factor(),
    materialCat = ifelse(materialCat %in% c("n/a", ""), "other", materialCat) %>% as.factor(),
    Shape = ifelse(Shape %in% c("round", "roude"), "round",
                   ifelse(Shape %in% c("oval", "ovale"), "oval",
                          ifelse(Shape == "squ_rect", "squ_rect", "other"))) %>% as.factor(),
    artistliving = as.factor(artistliving),
    diff_origin = as.factor(diff_origin),
    engraved = as.factor(engraved),
    original = as.factor(original),
    prevcoll = as.factor(prevcoll),
    othartist = as.factor(othartist),
    paired = as.factor(paired),
    figures = as.factor(figures),
    lrgfont = as.factor(lrgfont),
    relig = as.factor(relig),
    landsALL = as.factor(landsALL),
    arch = as.factor(arch),
    mytho = as.factor(mytho),
    peasant = as.factor(peasant),
    othgenre = as.factor(othgenre),
    singlefig = as.factor(singlefig),
    portrait = as.factor(portrait),
    still_life = as.factor(still_life),
    discauth = as.factor(discauth),
    history = as.factor(history),
    allegory = as.factor(allegory),
    pastorale = as.factor(pastorale),
    finished = as.factor(finished)
  ) %>%  
  .[,c(8, 1:7, 9:38)]

```

## Package Imputation
```{r message=FALSE, warning=FALSE, echo=FALSE}
micetest = mice::mice(paintings_train_1, printFlag = FALSE)
paintings_train_2 = mice::complete(micetest) %>% 
  mutate(Interm = as.factor(Interm))

```


## Plots 
```{r, message=FALSE, warning=FALSE, echo=FALSE}
graph_numeric = paintings_train_2 %>% 
  dplyr::select(position,
         year,
         Surface,
         nfigures)

graph_categorical = paintings_train_2 %>% 
  dplyr::select(-position,
         -year,
         -Surface,
         -nfigures,
         -logprice)
```

```{r}
## numeric
par(mfrow = c(2, 2))
for (i in 1:ncol(graph_numeric)){
  plot(y = paintings_train_2$logprice, 
       x = graph_numeric[,i],
       ylab = "logprice",
       xlab = names(graph_numeric)[i])
}
```

```{r}
## categorical
par(mfrow = c(2,3))
for (i in 1:ncol(graph_categorical)){
  boxplot(paintings_train_2$logprice ~ graph_categorical[,i],
          ylab = "logprice",
          xlab = names(graph_categorical)[i])
}

```


### Build your first model
## BMA variable selection
```{r}
## JZS prior
bma1 = bas.lm(logprice~ ., 
             data=paintings_train_2, 
             method="MCMC", 
             prior = "JZS",
             modelprior=uniform(),
             n.models = 15000, MCMC.iterations=100000, 
             thin = 10, initprobs="marg-eplogp",
             force.heredity=FALSE)
plot(bma1, which=4)
BPM1 = predict(bma1, estimator = "BPM")
variable.names(BPM1)

## g-prior
bma2 = bas.lm(logprice~ ., 
             data=paintings_train_2, 
             method="MCMC", 
             prior = "JZS",
             modelprior=uniform(),
             n.models = 15000, MCMC.iterations=100000, 
             thin = 10, initprobs="marg-eplogp",
             force.heredity=FALSE)

plot(bma2, which=4)
BPM2 = predict(bma2, estimator = "BPM")
variable.names(BPM2)
```

## OLS
```{r}
ols = lm(logprice ~ (dealer + school_pntg + diff_origin + artistliving + endbuyer + authorstyle + 
                       Interm + Shape + Surface + engraved + prevcoll + paired + 
                       finished + lrgfont + portrait + discauth + still_life)^2, 
         data = paintings_train_2)
summary(ols)
plot(ols)
```

## AIC selection
```{r, echo=FALSE}
AIC.ols = step(ols, k = 2)
summary(AIC.ols)
```

## BIC selection
```{r, echo=FALSE}
n = nrow(paintings_train_2)
BIC.ols = step(ols, k = log(n))
summary(BIC.ols)
```

## Combine AIC/BIC for OLS
```{r}
ols.2 = lm(logprice ~ Shape + school_pntg + dealer*Interm + dealer*Surface + dealer*paired + dealer*finished + diff_origin*Surface + diff_origin*portrait + artistliving*endbuyer + Interm*Surface + Interm*lrgfont + Surface*lrgfont + Surface*still_life + Surface*discauth + prevcoll*finished + paired*lrgfont + paired*discauth + diff_origin*authorstyle + diff_origin*still_life + finished*discauth + lrgfont*discauth + artistliving*finished + Interm*portrait + dealer*artistliving + authorstyle*prevcoll, data = paintings_train_2)

summary(ols.2)

par(mfrow = c(2,2))
plot(ols.2)

## table
table_of_coef = exp(cbind(coef(ols.2), confint(ols.2)))
colnames(table_of_coef) = c("Coefficient", "2.5%", "97.5%")
kable(table_of_coef)
variable.names(ols.2)
```

## Clean Test Data
```{r, echo=FALSE}
paintings_test_1 = paintings_test %>% 
  dplyr::select(-sale,
         -price,
         -count,
         -subject,
         -authorstandard,
         -author,
         -winningbidder,
         -other,
         -Height_in,
         -Width_in,
         -Surface_Rect,
         -Diam_in,
         -Surface_Rnd,
         -material,
         -mat,
         -lands_sc,
         -lands_elem,
         -lands_figs,
         -lands_ment,
         -lot,
         -type_intermed) %>% 
  mutate(
    dealer = as.factor(dealer),
    origin_author = as.factor(origin_author),
    origin_cat = as.factor(origin_cat),
    school_pntg = as.factor(school_pntg),
    authorstyle = ifelse(authorstyle %in% c("n/a", ""), 0, 1) %>% as.factor(),
    winningbiddertype = ifelse(winningbiddertype %in% c("n/a", ""), "X", winningbiddertype) %>% as.factor(),
    endbuyer = ifelse(endbuyer %in% c("n/a", ""), "X", endbuyer) %>% as.factor(),
    #type_intermed = ifelse(type_intermed %in% c("n/a", ""), "X", type_intermed) %>% as.factor(),
    materialCat = ifelse(materialCat %in% c("n/a", ""), "other", materialCat) %>% as.factor(),
    Shape = ifelse(Shape %in% c("round", "roude"), "round",
                   ifelse(Shape %in% c("oval", "ovale"), "oval",
                          ifelse(Shape == "squ_rect", "squ_rect", "other"))) %>% as.factor(),
    artistliving = as.factor(artistliving),
    diff_origin = as.factor(diff_origin),
    engraved = as.factor(engraved),
    original = as.factor(original),
    prevcoll = as.factor(prevcoll),
    othartist = as.factor(othartist),
    paired = as.factor(paired),
    figures = as.factor(figures),
    lrgfont = as.factor(lrgfont),
    relig = as.factor(relig),
    landsALL = as.factor(landsALL),
    arch = as.factor(arch),
    mytho = as.factor(mytho),
    peasant = as.factor(peasant),
    othgenre = as.factor(othgenre),
    singlefig = as.factor(singlefig),
    portrait = as.factor(portrait),
    still_life = as.factor(still_life),
    discauth = as.factor(discauth),
    history = as.factor(history),
    allegory = as.factor(allegory),
    pastorale = as.factor(pastorale),
    finished = as.factor(finished)
  )

```


```{r, echo=FALSE}
micetest.2 = mice::mice(paintings_test_1, printFlag = FALSE)

paintings_test_2 = mice::complete(micetest.2) %>% 
  mutate(Interm = as.factor(Interm))

```


## Save predictions and intervals.  
```{r predict-model1, echo=FALSE, eval=FALSE}
predictions = as.data.frame(
  exp(predict(ols.2, newdata=paintings_test_2, 
              interval = "pred")))

save(predictions, file="predict-test.Rdata")
```

#1. Introduction: Summary of problem and objectives

In this study, the auction prices of paintings in 18th century Paris were examined. Specifically, we wish to understand the variables which affect the prices of the paintings, and then be able to predict auction prices based on characteristics of a certain painting. By fitting an appropriate model, we will also be creating a tool to help decide whether specific paintings that are either underpriced or overpriced given their realization of the covariates that were included in the model.

One of the main challenges in building this model is to narrow down the number of covariates from the 59 canadidates in the original data set to less than 20 in the final model. This must be done in such a way that an undue amount of bias is not introduced, and overfitting is avoided. The ability to explain the results and provide some recommendations to indivisuals without statistical background is equally important and challenging, since the primary audience for this analysis is intended to be art historians. The goal was therefore to balance predictive performance, model simplicity, and interprebility in order to create a pricing model for artwork in 18th century France.


#2. Exploratory data analysis (10 points): must include three correctly labeled graphs and an explanation that highlight the most important features that went into your model building.

## A) Data summary & cleaning
To start with, we looked at the summary of the original trainig data. There are few numeric variables and a lot of binary variables. Some variables, such as `Interm`, `Surface`, `Height_in` etc. have mising values, which need to be taken care of. The followings steps are how we cleaned the data: 

a. The first step we did was to get rid of intuitivelly useless variables to reduce dimention, including: `lot`, `sale`, `price`, `count`, `subject`, `authorstandard`, `author`, `winningbidder`, and `other`. From the summary table, the `count` variable has all 1's; the `other` variable does not convey useful information; the other variables, such as `names` and `subjects`, are not useful in predicting the response variable (such as names). From the table of unique values we can see that some variables have thousands of unique values. Therefore, we can remove them in the first step.

b. By further screening the variables, we found out that `Surface` and `Surface_Rnd`, `Surface_Rect` are corerlated, which are based on the value of `Height_in`, `Width_in`, and `Diam_in`. We decided to use `Surface` in our initial model. The same issue happened to `material`, `mat`, and `materialCat`. The latter one recodes the previous one. Therefore, we used `materialCat`. We applied the same strategy to keep `landsALL` and get rid of other variables related with landscape. 

c. For those variables that have multiple levels, to be consistent with how the data was originally coded, we recoded the missing levels as "X", which stands for "no information". For `materialCat` and `Shape`, since there are so many levels, we grouped some levels with few observations together, coded as "other" group. The rest binary vairables are changed into factor.

d. Then we dealt with the missing values in `Surface` and `Interm`. We used the package "mice" to address this problem, which uses the observed values in the dataset to impute the missing values. It prevents directly throwing away the missing values, which results in lossing a large amont of information for prediction.

## B). Plots
Then we analyed the relationship between those left features and the response variable. With the scatter plots, we can roughly determine which variables can be put into the initial model. For categorical variables, we want to check if the `logprice` spans different ranges in different levels. For numeric variables, we want to check if there is a clear relationship between them and `logprice`.  

For numeric variables, we see that `Surface` and `nfigures` seem to show some weak but positive relationship with `logprice`. Since there are several extremely large values in `position` (potentially outliers), it is hard to see that real pattern between the majority of points and `logprice`. But we'll keep it in the model first.

Since there are 33 categorical variables, we don't show the boxplots for all of them. But applied the same method to check all the categorical variables. The following variables show some differences in `logprice` at different levels (not considering the magnitude of the difference at this time): `dealer`, `origin_author`, `origin_cat`, `school_pntg`, `diff_origin`, `authorstyle`, `endbuyer`, `Interm`, `Shape`, `materialCat`, `engraved`, `prevcoll`, `figures`, `finished`, `Irgfont`, `othgenre`, `discauth`, and `still_life`.

If we were to choose 10 best predictive variables for predicting, we would consider the magnitude of differences and the strength of relationships. The 10 variables we choose are: `Surface`, `dealer`, `school_pntg`, `diff_origin`, `authorstyle`, `endbuyer`, `Interm`, `prevcoll`, `engraved`, `Irgfont`.


#3. Development and assessment of an initial model (10 points)

* Initial model: 

The EDA process gives us an initial idea of which variables to drop out to reduce the dimension, and which variables might be significant in explaining the variation in logprice. But before we built the initial model, we applied BMA, Bayesian Model Averaging, to systemetically choose which base variables that have higher posterior probabilities to be in the initial model. We experimented two modelpriors, "JZS" and "g-prior", which gave us two sets of variables listed above. Then we picked up the common ones from Best Predictive Model(BPM).

Then we fit the linear regression model using the chosen features and all their possible interactions. From the summary table, the $R^{2} = 0.5828$, which is fairly high. But we realized that lots of estimated coefficients for interactions are NAs, indicating that some levels in those variables have too few observations to be estimated. Therefore, we need to further reduce the dimention through variable selection.

* Model selection:

After completing the initial exploratory data analysis, methods including Stepwise Best Subset Selection using both AIC and BIC were used in order to assess more systematically which covariates were most important for predicting the logprice of paintings. While the number of relevant covariates was initially thinned by examining the data and determining which variables were best suited for modeling (e.g. via dimension reduction, elimination or recoding of categorical variables with too many levels or too few observations for a given level to be useful in estimating a coefficient), there still remained a large number of covariates from which to choose. The goal in using the above described methodology was to demonstrate among several methods, both frequentist and Bayesian, which covariates were routinely deemed to be the most important for modeling logprice. 

The variable selection methods described above remain computationally intensive, particularly given the number of variables and potential two-way interactions that must be considered. In order to begin the analysis, The two-way interactions were considered using stepwise selection (AIC & BIC). The goal of this penalized selection process was to avoid overfitting and to deliver a model that was both interpretable and performed well in prediction. Then we compared the results from two methods and filtered out interactions that have NAs as coefficients, that are not significant, and that do not make sense to be interacted (such as $artistling * endbuyer$).

Ultimately, the following variables were selected using the above methods and were fit using OLS regression. The $R^{2}$ reduces to 0.60, which is expected. All the estimated coefficients do not contain NAs. 

* Residual: 
After fitting the model, we created the four model diagnostic plots. The overall appearances of all four plots seem acceptable, with no obvious outlier or highly influential points shown. The model also does not violate the normality assumption. The constant variance of residuals assumption seems to be satisfied. However, there are 2 cases that are dropped from the plots because they both have leverage of 1, indicating that they could potentially be the outlying cases of underpriced/overpriced paintings that we will later on investigate in, or have extreme price values. It is worth our attention to specifically look at these cases. 

* Variables: 
In the linear model we selected, we included `Shape`, `school_pntg`, `dealer`, `Interm`, `Surface`, `paired`, `finished`, `discauth`, `diff_origin`, `portrait`, `artistliving`, `endbuyer`, `authorstyle`, `lrgfont`, `still_life`, and `prevcoll` as our base predictors. Interactions selected by the model selection process and, for the sake of interpretation, those that are reasonable and interpretable are kept in the model as well. Since the response variable was orginally log-transformed, the exponentiated coefficients and confidence intervals are shown in the table.

#4. Summary and Conclusions (10 points)

The median price predicted is `exp(4.401232) = 81.55128` livres. The 95% confidence interval is that the price of an auctioned painting will, on average, be on the interval (6.248, 1064.357) livres. The prediction interval is that the price of a specific painting will be on the interval (2.532, 2626.714) livres.

## Interpretation
From the final model, the following variables are statistically significant: `dealer`, `Interm`, `Surface`, `finished`, `discauth`, `diff_origin`, `portrait`, `endbuyer (E,U, X)`, `authorstyle`, `lrgfont`, and `prevcoll`. Some of the interactions are statistically important, such as: `dealer*Interm`, `dealer*paried`, `Interm*lrgfont`, `diff_origin*portrait` etc. The most important covariates and interactions are interpreted as follows: 

* dealer: the type of dealer that the auction went through significantly affects the price of the painting. For example, compared with dealer J, the average price from dealer L is `244.5% higher`. (Same interpretation for dealer P and R, with different coefficients)

* Interm: when there is an intermediary involved in the transaction, on average, the selling price is `99% lower` than when there is no intermediary involved. 

* Surface: for every one square inch increase in the painting surface, the selling price is, on average expected to increase `.045%`.

* finished: if the painting is noted for being highly finished, the selling price on average is `130.3% higher` than when the painting is not noted for being highly finished.

* portrait: if the painting is described as a portrait, the selling price on average is `101.4% lower` times lower than when the painting is not described as a portrait. 

* endbuyer: the type of endbuyer will significantly affect the level of price. For instance, compared with the endbuyer type B (buyer), the average selling price is `-108.1% lower` when the endbuyer is type E (expert). 

* prevcoll: when the previous owner is mentioned, the average selling price is `151.0% higher` than when the previous owner is not mentioned.

* lrgfont: when the dealer devotes an additional paragraph, the average selling price is `178.0% higher` than when there is no additional paragraph.

* authorstyle: when the author's name is introduced, the average selling price is expected to be `200.4% lower` than when the author's name is not introduced.

* dealer&Interm interaction: when an intermediary is present, which the price of the auctioned paintings differs significantly among different dealers. For instance, if the dealer is R and an intermediary is used, the average selling price is `225.0% higher` than when the dealer is J with an intermediary.

* finished*prevcoll: given that the painting is noted for being highly finished, when the previous owner is mentioned, the average price is expected to be `110.4% lower` than when the previous owner is not mentioned.


## Recommendations
In order to understand the auction prices of 18th century paintings and predict prices of paintings with certain features, we recommend historians focusing on the characteristics mentioned above associated with the painitings (just to mention some, not a complete list). For example, in order to find out highly priced pieces, they might want to look for transactions that involved dealer R, with an intermediary involved; they might want to look for dealer L with the painting sold as a pair with another one; they should look for larger, finished paintings; they should focus on paintings whose authors' names are mentioned during the auction. These features were among those that conferred the greatest percent increase in price over the base case in the model presented above.


## Limitations
As mentioned in the data cleaning process, some of the variables have so many levels that fitting (and interpreting) such a model would be cumbersome. Many levels of the categorical variables included in this data set have few observations that are not sufficient for estimating coefficients. Therefore, we grouped some of the levels of variables, and grouped some variables via dimension reduction. This reduces the granularity of the analysis presented above, and risks introducing some bias, as the true model would probably be more granular and include the actual realization of each categorical predictor, not just an artbitrary grouping.

As our goal is to find a balance between the prediction accuracy and interpretability, our model may not predict the logprice response variable as accurately as other more advanced methods (which sacrifice interpretability in exchange for improved predictions). In the next phase of this project, we will attempt to fit other models which do just this, then compare the results to our initial work using an OLS regression model. 



