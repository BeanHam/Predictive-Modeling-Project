---
title: "Final Data Analysis Project"
output:
  html_document:
    df_print: paged
date: "See Parts for Write-Up due Dates"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

### Read in Training Data
```{r message=FALSE, warning=FALSE, echo =FALSE}
suppressWarnings(library(knitr)) 
suppressWarnings(library(dplyr))
suppressWarnings(library(ggplot2))
suppressWarnings(library(GGally))
suppressWarnings(library(mice))
suppressWarnings(library(purrr))
suppressWarnings(library(glmnet))
suppressWarnings(library(MASS))
suppressWarnings(library(BAS))
```


To get started read in the training data:
```{r read-data, echo=TRUE}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
```


## Part I: Simple Model 

### EDA
  
## Data Summary
```{r, echo=FALSE}
t(summary(paintings_train))
str(paintings_train)

```

## Manually Data Cleaning
```{r, echo=FALSE}
## Remove Intuitively Useless Variables
paintings_train_1 = paintings_train %>% 
  dplyr::select(-sale,
                -price,
                -count,
                -subject,
                -authorstandard,
                -author,
                -winningbidder,
                -other,
                -Height_in,
                -Width_in,
                -Surface_Rect,
                -Diam_in,
                -Surface_Rnd,
                -material,
                -mat,
                -lands_sc,
                -lands_elem,
                -lands_figs,
                -lands_ment,
                -lot,
                -type_intermed) %>% 
  mutate(
    dealer = as.factor(dealer),
    origin_author = as.factor(origin_author),
    origin_cat = as.factor(origin_cat),
    school_pntg = as.factor(school_pntg),
    authorstyle = ifelse(authorstyle %in% c("n/a", ""), 0, 1) %>% as.factor(),
    winningbiddertype = ifelse(winningbiddertype %in% c("n/a", ""), "X", winningbiddertype) %>% as.factor(),
    endbuyer = ifelse(endbuyer %in% c("n/a", ""), "X", endbuyer) %>% as.factor(),
    materialCat = ifelse(materialCat %in% c("n/a", ""), "other", materialCat) %>% as.factor(),
    Shape = ifelse(Shape %in% c("round", "roude"), "round",
                   ifelse(Shape %in% c("oval", "ovale"), "oval",
                          ifelse(Shape == "squ_rect", "squ_rect", "other"))) %>% as.factor(),
    artistliving = as.factor(artistliving),
    diff_origin = as.factor(diff_origin),
    engraved = as.factor(engraved),
    original = as.factor(original),
    prevcoll = as.factor(prevcoll),
    othartist = as.factor(othartist),
    paired = as.factor(paired),
    figures = as.factor(figures),
    lrgfont = as.factor(lrgfont),
    relig = as.factor(relig),
    landsALL = as.factor(landsALL),
    arch = as.factor(arch),
    mytho = as.factor(mytho),
    peasant = as.factor(peasant),
    othgenre = as.factor(othgenre),
    singlefig = as.factor(singlefig),
    portrait = as.factor(portrait),
    still_life = as.factor(still_life),
    discauth = as.factor(discauth),
    history = as.factor(history),
    allegory = as.factor(allegory),
    pastorale = as.factor(pastorale),
    finished = as.factor(finished)
  ) %>%  
  .[,c(8, 1:7, 9:38)]

```

## Package Imputation
```{r message=FALSE, warning=FALSE}
micetest = mice::mice(paintings_train_1)
paintings_train_2 = mice::complete(micetest) %>% 
  mutate(Interm = as.factor(Interm))

```


## Plots 
```{r, message=FALSE, warning=FALSE}
graph_numeric = paintings_train_2 %>% 
  dplyr::select(position,
         year,
         Surface,
         nfigures)

graph_categorical = paintings_train_2 %>% 
  dplyr::select(-position,
         -year,
         -Surface,
         -nfigures,
         -logprice)
```

```{r}
## numeric
par(mfrow = c(2, 2))
for (i in 1:ncol(graph_numeric)){
  plot(y = paintings_train_2$logprice, 
       x = graph_numeric[,i],
       ylab = "logprice",
       xlab = names(graph_numeric)[i])
}

```

```{r}
## categorical
par(mfrow = c(2,3))
for (i in 1:ncol(graph_categorical)){
  boxplot(paintings_train_2$logprice ~ graph_categorical[,i],
          ylab = "logprice",
          xlab = names(graph_categorical)[i])
}

```


### Build your first model
## BMA variable selection
```{r}
## JZS prior
bma1 = bas.lm(logprice~ ., 
             data=paintings_train_2, 
             method="MCMC", 
             prior = "JZS",
             modelprior=uniform(),
             n.models = 15000, MCMC.iterations=100000, 
             thin = 10, initprobs="marg-eplogp",
             force.heredity=FALSE)
plot(bma1, which=4)
BPM1 = predict(bma1, estimator = "BPM")
variable.names(BPM1)

## g-prior
bma2 = bas.lm(logprice~ ., 
             data=paintings_train_2, 
             method="MCMC", 
             prior = "JZS",
             modelprior=uniform(),
             n.models = 15000, MCMC.iterations=100000, 
             thin = 10, initprobs="marg-eplogp",
             force.heredity=FALSE)

plot(bma2, which=4)
BPM2 = predict(bma2, estimator = "BPM")
variable.names(BPM2)
```

## OLS
```{r}
ols = lm(logprice ~ (dealer + school_pntg + diff_origin + artistliving + endbuyer + authorstyle + 
                       Interm + Shape + Surface + engraved + prevcoll + paired + 
                       finished + lrgfont + portrait + discauth + still_life)^2, 
         data = paintings_train_2)
summary(ols)
plot(ols)
```

## AIC selection
```{r cache=TRUE}
AIC.ols = step(ols, k = 2)
summary(AIC.ols)
```

## BIC selection
```{r, cache=TRUE}
n = nrow(paintings_train_2)
BIC.ols = step(ols, k = log(n))
summary(BIC.ols)
```

## Combine AIC/BIC for OLS
```{r}
ols.2 = lm(logprice ~ Shape + school_pntg + dealer*Interm + dealer*Surface + dealer*paired + dealer*finished + dealer*discauth + diff_origin*Surface + diff_origin*portrait + artistliving*endbuyer + artistliving*authorstyle + Interm*Surface + Interm*lrgfont + Surface*lrgfont + Surface*still_life + Surface*discauth + prevcoll*finished + paired*lrgfont + paired*discauth + diff_origin*authorstyle + diff_origin*still_life + finished*discauth + lrgfont*discauth + artistliving*finished + Interm*portrait, data = paintings_train_2)

summary(ols.2)
par(mfrow = c(2,2))
plot(ols.2)

## table
table_of_coef = exp(cbind(coef(ols.2), confint(ols.2)))
colnames(table_of_coef) = c("Coefficient", "2.5%", "97.5%")
kable(table_of_coef)
variable.names(ols.2)
```

## Clean Test Data
```{r}
paintings_test_1 = paintings_test %>% 
  dplyr::select(-sale,
         -price,
         -count,
         -subject,
         -authorstandard,
         -author,
         -winningbidder,
         -other,
         -Height_in,
         -Width_in,
         -Surface_Rect,
         -Diam_in,
         -Surface_Rnd,
         -material,
         -mat,
         -lands_sc,
         -lands_elem,
         -lands_figs,
         -lands_ment,
         -lot,
         -type_intermed) %>% 
  mutate(
    dealer = as.factor(dealer),
    origin_author = as.factor(origin_author),
    origin_cat = as.factor(origin_cat),
    school_pntg = as.factor(school_pntg),
    authorstyle = ifelse(authorstyle %in% c("n/a", ""), 0, 1) %>% as.factor(),
    winningbiddertype = ifelse(winningbiddertype %in% c("n/a", ""), "X", winningbiddertype) %>% as.factor(),
    endbuyer = ifelse(endbuyer %in% c("n/a", ""), "X", endbuyer) %>% as.factor(),
    #type_intermed = ifelse(type_intermed %in% c("n/a", ""), "X", type_intermed) %>% as.factor(),
    materialCat = ifelse(materialCat %in% c("n/a", ""), "other", materialCat) %>% as.factor(),
    Shape = ifelse(Shape %in% c("round", "roude"), "round",
                   ifelse(Shape %in% c("oval", "ovale"), "oval",
                          ifelse(Shape == "squ_rect", "squ_rect", "other"))) %>% as.factor(),
    artistliving = as.factor(artistliving),
    diff_origin = as.factor(diff_origin),
    engraved = as.factor(engraved),
    original = as.factor(original),
    prevcoll = as.factor(prevcoll),
    othartist = as.factor(othartist),
    paired = as.factor(paired),
    figures = as.factor(figures),
    lrgfont = as.factor(lrgfont),
    relig = as.factor(relig),
    landsALL = as.factor(landsALL),
    arch = as.factor(arch),
    mytho = as.factor(mytho),
    peasant = as.factor(peasant),
    othgenre = as.factor(othgenre),
    singlefig = as.factor(singlefig),
    portrait = as.factor(portrait),
    still_life = as.factor(still_life),
    discauth = as.factor(discauth),
    history = as.factor(history),
    allegory = as.factor(allegory),
    pastorale = as.factor(pastorale),
    finished = as.factor(finished)
  )

```


```{r}

micetest.2 = mice::mice(paintings_test_1)

paintings_test_2 = mice::complete(micetest.2) %>% 
  mutate(Interm = as.factor(Interm))

```


## Save predictions and intervals.  
```{r predict-model1, echo=FALSE, eval=FALSE}
predictions = as.data.frame(
  exp(predict(ols.2, newdata=paintings_test_2, 
              interval = "pred")))

save(predictions, file="predict-test.Rdata")
```

Once you are satisfied with your model, provide a write up of your data analysis project in a new Rmd file/pdf file: `Part-I-Writeup.Rmd` by copying over salient parts of your R notebook. The written assignment consists of five parts:

#1. Introduction: Summary of problem and objectives (5 points)

In this study, we are looking at the auction prices of paintings in 18th century Paris. Specifically, through the assistance of model built based on existing training data, we wish to understand the factors that drive the prices of the paintings, and then be able to predict auction prices based on characteristics of a certain painting. After fitting appropriate model, we also intend to detect specific paintings that are either underpriced or overpriced based on the selected model.
One of the main task and challenge is to narrow down the number of potential predictors from 59 to less than 20 while maintaining a high performance of the model. We also aim at balancing the performance of model prediction, closeness to true model, simplicity and interprebility.


#2. Exploratory data analysis (10 points): must include three correctly labeled graphs and an explanation that highlight the most important features that went into your model building.

#A). Data summary & cleaning
To start with, we looked at the summary of the original trainig data. There are few numeric variables and a lot of binary variables. Some variables, such as $Interm$, $Surface$, $Height_in$ etc. have mising values, which need to be taken care of. The followings steps are how we cleaned the data: 

a. The first step we did was to get rid of intuitivelly not helpful variables, including: $lot$, $sale$, $price$, $count$, $subject$, $authorstandard$, $author$, $winningbidder$, and $other$. The are not useful in predicting the response variable (such as names)

b. By further screening the variables, we found out that $Surface$ and $Surface_Rnd$, $Surface_Rect$ are corerlated, which are based on the value of $Height_in$, $Width_in$, and $Diam_in$. We decided to use $Surface$. The same issue happened to $material$, $mat$, and $materialCat$. The latter one just recodes the previous one. Therefore, we used $materialCat$. We applied the same strategy to keep $landsALL$ and get rid of other variables related with land. 

c. For those variables that have multiple levels, to be consistent with how the data was originally coded, we recoded the missing levels as "X", which stands for "no information". For $materialCat$ and $Shape$, since there are so many levels, we grouped some levels with few observations together, coded as "other" group. The rest binary vairables are changed into factor.

d. The most important thing left was to deal with the missing values in $Surface$ and $Interm$. We used the package "mice", which uses the observed values in the dataset to impute the missing values, to address the problem. It prevents directly throwing away the missing values, which results in lossing a large amont of information for prediction.

#B). Plots
Then we analyed the relationship between those left features and the response variable. With the scatter plots, we can roughly determine which variables can be put into the initial model. For categorical variables, we want to check if the $logprice$ spans different ranges in different levels. For numeric variables, we want to check if there is a clear relationship between them and $logprice$.  

For numeric variables, we see that $Surface$ and $nfigures$ seem to show some weak but positive relationship with $logprice$. Since there are several extremely large values in $position$ (potentially outliers), it is hard to see that real pattern between the majority of points and $logprice$. But we'll keep it in the model first.

For categorical variables, the following variables show some differences in $logprice$ at different levels (not considering the magnitude of the difference at this time): $dealer$, $origin_author$, $origin_cat$, $school_pntg$, $diff_origin$, $authorstyle$, $endbuyer$, $Interm$, $Shape$, $materialCat$, $engraved$, $prevcoll$, $figures$, $finished$, $Irgfont$, $othgenre$, $discauth$, and $still_life$.

If we were to choose 10 best predictive variables for predicting, we would consider the magnitude of differences and the strength of relationships. The 10 variables we choose are: $Surface$, $dealer$, $school_pntg$, $diff_origin$, $authorstyle$, $endbuyer$, $Interm$, $prevcoll$, $engraved$, $Irgfont$.


#3. Development and assessment of an initial model (10 points)

* Initial model: must include a summary table and an explanation/discussion for variable selection and overall amount of variation explained. 

* Model selection: must include a discussion

After completing the initial exploratory data analysis, methods including Bayesian Model Averaging and Stepwise Best Subset Selection using both AIC and BIC were used in order to assess more systematically which covariates were most important for predicting the logprice of paintings. While the number of relevant covariates was initially thinned by examining the data and determining which variables were best suited for modeling (e.g. via dimension reduction, elimination or recoding of categorical variables with too many levels or too few observations for a given level to be useful in estimating a coefficient), there still remained a large number of covariates from which to choose. The goal in using the above described methodology was to demonstrate among several methods, both frequentist and Bayesian, which covariates were routinely deemed to be the most important for modeling logprice. 

The variable selection methods described above remain computationally intensive, particularly given the number of variables and potential two-way interactions that must be considered. In order to begin the analysis, Bayesian Model Averaging was used with MCMC in order to explore the space of possible models more efficiently. The covariates with the highest posterior inclusion probabilities, and their two-way interactions, were then considered using stepwise selection. The goal of this penalized selection process was to avoid overfitting and to deliver a model that was both interpretable and performed well in prediction.

Ultimately, the following variables were selected using the above methods and were fit using OLS regression:

**insert table with final model coeff here**

Further discussion about positive results and potential issues with model selection method described above?

* Residual: must include residual plot(s) and a discussion: After fitting the model, we created the four model diagnostic plots. The overall appearances of all four plots seem acceptable, with no obvious outlier or highly influential points shown. The model also does not violate the normality assumption. However, there are 2 cases that are dropped from the plots because they both have leverage of 1, indicating that they could potentially be the outlying cases of underpriced/overpriced paintings that we will later on investigate in, or have extreme price values. It is worth our attention to specifically look at these cases. 

* Variables: In the linear model we selected, we included Shape, school_pntg, dealer, Interm, Surface paired, finished, discauth, diff_origin, portrait, artistliving, endbuyer, authorstyle, lrgfont, still_life and prevcoll as our base predictors. Interactions selected by the model selection process and, for the sake of interpretation, those that are reasonable and interpretable are kept in the model as well. Since the response variable was orginally log-transformed, the exponentiated coefficients and confidence intervals are shown in the table.

#4. Summary and Conclusions (10 points)

What is the (median) price for the "baseline" category if there are categorical or dummy variables in the model (add CI's)?  (be sure to include units!) Highlight important findings and potential limitations of your model.  Does it appear that interactions are important?  What are the most important variables and/or interactions?  Provide interprations of how the most important variables influence the (median) price giving a range (CI).  Correct interpretation of coefficients for the log model desirable for full points.

Provide recommendations for the art historian about features or combination of features to look for to find the most valuable paintings.


